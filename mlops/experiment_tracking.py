#!/usr/bin/env python3
"""
Experiment Tracking and Model Versioning for Medical Vision Transformer
Includes Weights & Biases integration, DVC for data versioning, and MLflow for model registry
"""

import os
import json
import yaml
import hashlib
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Optional imports for advanced tracking
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("‚ö†Ô∏è Weights & Biases not available. Install with: pip install wandb")

try:
    import mlflow
    import mlflow.pytorch
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("‚ö†Ô∏è MLflow not available. Install with: pip install mlflow")

try:
    import dvc.api
    DVC_AVAILABLE = True
except ImportError:
    DVC_AVAILABLE = False
    print("‚ö†Ô∏è DVC not available. Install with: pip install dvc")


class ExperimentTracker:
    """Comprehensive experiment tracking system"""
    
    def __init__(self, project_name="medical-vit", config=None):
        self.project_name = project_name
        self.config = config or {}
        self.experiments_dir = Path("experiments")
        self.experiments_dir.mkdir(exist_ok=True)
        
        # Initialize tracking systems
        self.wandb_run = None
        self.mlflow_run = None
        self.experiment_id = self._generate_experiment_id()
        
        # Create experiment directory
        self.experiment_dir = self.experiments_dir / self.experiment_id
        self.experiment_dir.mkdir(exist_ok=True)
        
        # Experiment metadata
        self.metadata = {
            'experiment_id': self.experiment_id,
            'timestamp': datetime.now().isoformat(),
            'project_name': project_name,
            'config': self.config,
            'metrics': {},
            'artifacts': [],
            'tags': []
        }
    
    def _generate_experiment_id(self):
        """Generate unique experiment ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_suffix = hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]
        return f"{self.project_name}_{timestamp}_{random_suffix}"
    
    def start_wandb(self, config=None, tags=None):
        """Start Weights & Biases tracking"""
        if not WANDB_AVAILABLE:
            print("‚ùå Weights & Biases not available")
            return False
        
        try:
            wandb_config = {
                'project': self.project_name,
                'name': self.experiment_id,
                'tags': tags or [],
                'config': config or self.config
            }
            
            self.wandb_run = wandb.init(**wandb_config)
            print(f"‚úÖ Weights & Biases tracking started: {self.wandb_run.url}")
            return True
        except Exception as e:
            print(f"‚ùå Failed to start Weights & Biases: {e}")
            return False
    
    def start_mlflow(self, experiment_name=None):
        """Start MLflow tracking"""
        if not MLFLOW_AVAILABLE:
            print("‚ùå MLflow not available")
            return False
        
        try:
            experiment_name = experiment_name or self.project_name
            mlflow.set_experiment(experiment_name)
            
            self.mlflow_run = mlflow.start_run(run_name=self.experiment_id)
            print(f"‚úÖ MLflow tracking started: {self.mlflow_run.info.run_id}")
            return True
        except Exception as e:
            print(f"‚ùå Failed to start MLflow: {e}")
            return False
    
    def log_config(self, config):
        """Log configuration parameters"""
        self.metadata['config'] = config
        
        # Save config to experiment directory
        config_path = self.experiment_dir / "config.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # Log to tracking systems
        if self.wandb_run:
            wandb.config.update(config)
        
        if self.mlflow_run:
            for key, value in config.items():
                if isinstance(value, dict):
                    for sub_key, sub_value in value.items():
                        mlflow.log_param(f"{key}.{sub_key}", sub_value)
                else:
                    mlflow.log_param(key, value)
    
    def log_metrics(self, metrics, step=None):
        """Log metrics to all tracking systems"""
        self.metadata['metrics'].update(metrics)
        
        # Save metrics to experiment directory
        metrics_path = self.experiment_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.metadata['metrics'], f, indent=2)
        
        # Log to tracking systems
        if self.wandb_run:
            wandb.log(metrics, step=step)
        
        if self.mlflow_run:
            for key, value in metrics.items():
                mlflow.log_metric(key, value, step=step)
    
    def log_model(self, model, model_name="model", metadata=None):
        """Log model to tracking systems"""
        model_path = self.experiment_dir / f"{model_name}.pth"
        torch.save(model.state_dict(), model_path)
        
        # Add to artifacts
        self.metadata['artifacts'].append({
            'type': 'model',
            'name': model_name,
            'path': str(model_path),
            'metadata': metadata or {}
        })
        
        # Log to tracking systems
        if self.wandb_run:
            wandb.save(str(model_path))
            wandb.log_model(model, model_name)
        
        if self.mlflow_run:
            mlflow.pytorch.log_model(model, model_name, extra_files=[str(model_path)])
    
    def log_artifacts(self, file_paths, artifact_type="other"):
        """Log artifacts (images, plots, etc.)"""
        for file_path in file_paths:
            if os.path.exists(file_path):
                # Copy to experiment directory
                dest_path = self.experiment_dir / Path(file_path).name
                shutil.copy2(file_path, dest_path)
                
                # Add to artifacts
                self.metadata['artifacts'].append({
                    'type': artifact_type,
                    'name': Path(file_path).name,
                    'path': str(dest_path),
                    'original_path': str(file_path)
                })
                
                # Log to tracking systems
                if self.wandb_run:
                    wandb.save(file_path)
                
                if self.mlflow_run:
                    mlflow.log_artifact(file_path)
    
    def log_plots(self, plots_dict):
        """Log matplotlib plots"""
        for plot_name, fig in plots_dict.items():
            plot_path = self.experiment_dir / f"{plot_name}.png"
            fig.savefig(plot_path, dpi=300, bbox_inches='tight')
            
            # Log to tracking systems
            if self.wandb_run:
                wandb.log({plot_name: wandb.Image(str(plot_path))})
            
            if self.mlflow_run:
                mlflow.log_artifact(str(plot_path))
    
    def add_tags(self, tags):
        """Add tags to experiment"""
        if isinstance(tags, str):
            tags = [tags]
        
        self.metadata['tags'].extend(tags)
        
        if self.wandb_run:
            wandb.run.tags = wandb.run.tags + tags
    
    def finish(self):
        """Finish experiment tracking"""
        # Save final metadata
        metadata_path = self.experiment_dir / "metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(self.metadata, f, indent=2)
        
        # Finish tracking systems
        if self.wandb_run:
            wandb.finish()
            print("‚úÖ Weights & Biases tracking finished")
        
        if self.mlflow_run:
            mlflow.end_run()
            print("‚úÖ MLflow tracking finished")
        
        print(f"üìÅ Experiment saved to: {self.experiment_dir}")


class ModelVersioning:
    """Model versioning and registry system"""
    
    def __init__(self, registry_path="model_registry"):
        self.registry_path = Path(registry_path)
        self.registry_path.mkdir(exist_ok=True)
        
        self.registry_file = self.registry_path / "registry.json"
        self.registry = self._load_registry()
    
    def _load_registry(self):
        """Load model registry"""
        if self.registry_file.exists():
            with open(self.registry_file, 'r') as f:
                return json.load(f)
        return {"models": [], "versions": {}}
    
    def _save_registry(self):
        """Save model registry"""
        with open(self.registry_file, 'w') as f:
            json.dump(self.registry, f, indent=2)
    
    def register_model(self, model_name, model_path, metadata=None, tags=None):
        """Register a new model version"""
        model_id = f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        model_info = {
            'model_id': model_id,
            'model_name': model_name,
            'model_path': str(model_path),
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {},
            'tags': tags or [],
            'status': 'registered'
        }
        
        # Add to registry
        self.registry['models'].append(model_info)
        
        if model_name not in self.registry['versions']:
            self.registry['versions'][model_name] = []
        
        self.registry['versions'][model_name].append(model_id)
        
        # Save registry
        self._save_registry()
        
        print(f"‚úÖ Model registered: {model_id}")
        return model_id
    
    def get_model_versions(self, model_name):
        """Get all versions of a model"""
        return self.registry['versions'].get(model_name, [])
    
    def get_latest_model(self, model_name):
        """Get the latest version of a model"""
        versions = self.get_model_versions(model_name)
        if versions:
            return versions[-1]
        return None
    
    def promote_model(self, model_id, stage="production"):
        """Promote model to a specific stage"""
        for model in self.registry['models']:
            if model['model_id'] == model_id:
                model['stage'] = stage
                model['promoted_at'] = datetime.now().isoformat()
                break
        
        self._save_registry()
        print(f"‚úÖ Model {model_id} promoted to {stage}")
    
    def list_models(self, stage=None):
        """List all models, optionally filtered by stage"""
        models = self.registry['models']
        if stage:
            models = [m for m in models if m.get('stage') == stage]
        
        return models


class DataVersioning:
    """Data versioning using DVC"""
    
    def __init__(self, data_path="data"):
        self.data_path = Path(data_path)
        self.dvc_available = DVC_AVAILABLE
    
    def add_data(self, data_path, message="Add data"):
        """Add data to DVC tracking"""
        if not self.dvc_available:
            print("‚ùå DVC not available for data versioning")
            return False
        
        try:
            # This would typically use DVC commands
            print(f"üìä Data versioning: {data_path}")
            return True
        except Exception as e:
            print(f"‚ùå Data versioning failed: {e}")
            return False
    
    def get_data_info(self, data_path):
        """Get data version information"""
        if not self.dvc_available:
            return {"status": "DVC not available"}
        
        try:
            # This would use DVC API to get data info
            return {"status": "tracked", "path": str(data_path)}
        except Exception as e:
            return {"status": "error", "error": str(e)}


class MLOpsPipeline:
    """Complete MLOps pipeline"""
    
    def __init__(self, config):
        self.config = config
        self.tracker = ExperimentTracker(config.get('project_name', 'medical-vit'))
        self.model_registry = ModelVersioning()
        self.data_versioning = DataVersioning()
        
        # Pipeline stages
        self.stages = {
            'data_preparation': False,
            'model_training': False,
            'model_evaluation': False,
            'model_deployment': False
        }
    
    def start_pipeline(self, tags=None):
        """Start the MLOps pipeline"""
        print("üöÄ Starting MLOps Pipeline...")
        
        # Start experiment tracking
        self.tracker.start_wandb(config=self.config, tags=tags)
        self.tracker.start_mlflow()
        
        # Log initial configuration
        self.tracker.log_config(self.config)
        
        print("‚úÖ MLOps pipeline started")
    
    def data_preparation_stage(self, data_info):
        """Data preparation stage"""
        print("üìä Data Preparation Stage...")
        
        # Log data information
        self.tracker.log_metrics({
            'data_samples': data_info.get('total_samples', 0),
            'data_classes': data_info.get('num_classes', 0),
            'data_size_mb': data_info.get('size_mb', 0)
        })
        
        # Version data
        self.data_versioning.add_data("data/chest_xray")
        
        self.stages['data_preparation'] = True
        print("‚úÖ Data preparation completed")
    
    def model_training_stage(self, model, training_metrics):
        """Model training stage"""
        print("ü§ñ Model Training Stage...")
        
        # Log training metrics
        self.tracker.log_metrics(training_metrics)
        
        # Log model
        self.tracker.log_model(model, "trained_model", {
            'parameters': sum(p.numel() for p in model.parameters()),
            'training_completed': True
        })
        
        # Register model
        model_id = self.model_registry.register_model(
            "medical_vit", 
            "models/best_model.pth",
            metadata=training_metrics,
            tags=["trained", "medical", "vit"]
        )
        
        self.stages['model_training'] = True
        print("‚úÖ Model training completed")
        return model_id
    
    def model_evaluation_stage(self, evaluation_metrics, plots):
        """Model evaluation stage"""
        print("üìà Model Evaluation Stage...")
        
        # Log evaluation metrics
        self.tracker.log_metrics(evaluation_metrics)
        
        # Log plots
        self.tracker.log_plots(plots)
        
        # Log artifacts
        artifact_paths = [
            "results/confusion_matrix.png",
            "results/roc_curves.png",
            "results/pr_curves.png"
        ]
        self.tracker.log_artifacts(artifact_paths, "evaluation")
        
        self.stages['model_evaluation'] = True
        print("‚úÖ Model evaluation completed")
    
    def model_deployment_stage(self, model_id, deployment_info):
        """Model deployment stage"""
        print("üöÄ Model Deployment Stage...")
        
        # Promote model to production
        self.model_registry.promote_model(model_id, "production")
        
        # Log deployment information
        self.tracker.log_metrics({
            'deployment_timestamp': datetime.now().isoformat(),
            'deployment_status': 'success'
        })
        
        self.stages['model_deployment'] = True
        print("‚úÖ Model deployment completed")
    
    def finish_pipeline(self):
        """Finish the MLOps pipeline"""
        print("üèÅ Finishing MLOps Pipeline...")
        
        # Log pipeline completion
        self.tracker.log_metrics({
            'pipeline_completed': True,
            'stages_completed': sum(self.stages.values()),
            'total_stages': len(self.stages)
        })
        
        # Finish tracking
        self.tracker.finish()
        
        print("‚úÖ MLOps pipeline completed successfully!")


def create_mlops_demo():
    """Create MLOps demonstration"""
    print("üîß Creating MLOps demonstration...")
    
    # Create sample data
    np.random.seed(42)
    
    # Simulate experiment tracking data
    epochs = np.arange(50)
    train_loss = 0.8 * np.exp(-epochs * 0.1) + 0.2 + np.random.normal(0, 0.02, 50)
    val_loss = 0.7 * np.exp(-epochs * 0.08) + 0.25 + np.random.normal(0, 0.02, 50)
    train_acc = 0.2 + 0.7 * (1 - np.exp(-epochs * 0.08)) + np.random.normal(0, 0.01, 50)
    val_acc = 0.15 + 0.75 * (1 - np.exp(-epochs * 0.06)) + np.random.normal(0, 0.01, 50)
    
    # Create visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('üîß MLOps Pipeline Demonstration', fontsize=16, fontweight='bold')
    
    # Training curves
    axes[0, 0].plot(epochs, train_loss, label='Train Loss', color='blue', linewidth=2)
    axes[0, 0].plot(epochs, val_loss, label='Val Loss', color='red', linewidth=2)
    axes[0, 0].set_title('Training Progress Tracking', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Accuracy curves
    axes[0, 1].plot(epochs, train_acc, label='Train Acc', color='blue', linewidth=2)
    axes[0, 1].plot(epochs, val_acc, label='Val Acc', color='red', linewidth=2)
    axes[0, 1].set_title('Accuracy Tracking', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Model registry
    model_names = ['Model v1.0', 'Model v1.1', 'Model v1.2', 'Model v2.0']
    model_accuracies = [0.85, 0.87, 0.89, 0.92]
    model_stages = ['archived', 'staging', 'production', 'production']
    
    colors = ['gray', 'orange', 'green', 'green']
    bars = axes[1, 0].bar(model_names, model_accuracies, color=colors, alpha=0.7)
    axes[1, 0].set_title('Model Registry & Versioning', fontsize=12, fontweight='bold')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # Add value labels on bars
    for bar, acc in zip(bars, model_accuracies):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.005,
                       f'{acc:.2f}', ha='center', va='bottom', fontweight='bold')
    
    # MLOps pipeline stages
    stages = ['Data Prep', 'Training', 'Evaluation', 'Deployment']
    completion = [100, 100, 100, 85]  # Deployment in progress
    
    bars = axes[1, 1].bar(stages, completion, color=['blue', 'green', 'orange', 'red'], alpha=0.7)
    axes[1, 1].set_title('MLOps Pipeline Progress', fontsize=12, fontweight='bold')
    axes[1, 1].set_ylabel('Completion (%)')
    axes[1, 1].set_ylim(0, 100)
    
    # Add value labels
    for bar, comp in zip(bars, completion):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 2,
                       f'{comp}%', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('results/mlops_demo.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("‚úÖ MLOps demonstration created!")


def main():
    """Test MLOps components"""
    print("üß™ Testing MLOps Components...")
    
    # Test experiment tracking
    print("Testing Experiment Tracker...")
    tracker = ExperimentTracker("test_project")
    tracker.log_config({"learning_rate": 0.001, "batch_size": 32})
    tracker.log_metrics({"accuracy": 0.95, "loss": 0.05})
    tracker.finish()
    print("‚úÖ Experiment Tracker working")
    
    # Test model versioning
    print("Testing Model Versioning...")
    model_registry = ModelVersioning()
    model_id = model_registry.register_model("test_model", "test_path.pth")
    print("‚úÖ Model Versioning working")
    
    # Test data versioning
    print("Testing Data Versioning...")
    data_versioning = DataVersioning()
    data_versioning.add_data("test_data")
    print("‚úÖ Data Versioning working")
    
    # Create demonstration
    create_mlops_demo()
    
    print("üéâ All MLOps components working!")


if __name__ == "__main__":
    main()
